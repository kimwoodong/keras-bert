{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dCpvgG0vwXAZ"
   },
   "source": [
    "# Predicting Movie Review Sentiment with BERT on TF Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xiYrZKaHwV81"
   },
   "source": [
    "If you’ve been following Natural Language Processing over the past year, you’ve probably heard of BERT: Bidirectional Encoder Representations from Transformers. It’s a neural network architecture designed by Google researchers that’s totally transformed what’s state-of-the-art for NLP tasks, like text classification, translation, summarization, and question answering.\n",
    "\n",
    "Now that BERT's been added to [TF Hub](https://www.tensorflow.org/hub) as a loadable module, it's easy(ish) to add into existing Tensorflow text pipelines. In an existing pipeline, BERT can replace text embedding layers like ELMO and GloVE. Alternatively, [finetuning](http://wiki.fast.ai/index.php/Fine_tuning) BERT can provide both an accuracy boost and faster training time in many cases.\n",
    "\n",
    "Here, we'll train a model to predict whether an IMDB movie review is positive or negative using BERT in Tensorflow with tf hub. Some code was adapted from [this colab notebook](https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb). Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hsZvic2YxnTz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0504 18:46:30.614220 10548 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import tensorflow_hub as hub\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from bert.tokenization import FullTokenizer\n",
    "from tqdm import tqdm_notebook\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# Initialize session\n",
    "sess = tf.Session()\n",
    "\n",
    "# Params for bert model and tokenization\n",
    "bert_path = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
    "max_seq_length = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/device:CPU:0', '/device:GPU:0']\n"
     ]
    }
   ],
   "source": [
    "## gpu 설정\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_devices():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos]\n",
    "print(get_available_devices()) \n",
    "\n",
    "## 정상 출력 시 \n",
    "## ['/device:CPU:0', '/device:GPU:0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cp5wfXDx5SPH"
   },
   "source": [
    "In addition to the standard libraries we imported above, we'll need to install BERT's python package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hhbGEfwgdEtw"
   },
   "outputs": [],
   "source": [
    "import bert\n",
    "from bert import run_classifier\n",
    "from bert import optimization\n",
    "from bert import tokenization\n",
    "\n",
    "from bert.tokenization import FullTokenizer\n",
    "from tqdm import tqdm_notebook\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pmFYvkylMwXn"
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MC_w8SRqN0fr"
   },
   "source": [
    "First, let's download the dataset, hosted by Stanford. The code below, which downloads, extracts, and imports the IMDB Large Movie Review Dataset, is borrowed from [this Tensorflow tutorial](https://www.tensorflow.org/hub/tutorials/text_classification_with_tf_hub)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2abfwdn-g135",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load all files from a directory in a DataFrame\n",
    "def load_directory_data(directory):\n",
    "    data = {}\n",
    "    data[\"sentence\"] = []\n",
    "    data[\"sentiment\"] = []\n",
    "    for file_path in os.listdir(directory):\n",
    "        print(file_path)\n",
    "        with open(os.path.join(directory, file_path),\"r\", encoding = \"utf-8\") as f:\n",
    "            print(os.path.join(directory, file_path))\n",
    "            data['sentence'].append(f.read())\n",
    "            data['sentiment'].append(re.match(\"\\d+_(\\d+)\\.txt\", file_path).group(1))\n",
    "            #data['sentiment'].append(file_path).group(1)\n",
    "            print(data)\n",
    "    return pd.DataFrame.from_dict(data)\n",
    "\n",
    "# Merge positive and negative examples, add a polarity column and shuffle.\n",
    "def load_dataset(directory):\n",
    "    pos_df = load_directory_data(os.path.join(directory, \"pos\"))\n",
    "    neg_df = load_directory_data(os.path.join(directory, \"neg\"))\n",
    "    pos_df['polarity'] = 1\n",
    "    neg_df['polarity'] = 0\n",
    "    \n",
    "    return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# dataset_dir = os.path.dirname('C:\\\\Users\\\\Ajou\\\\.keras\\\\datasets\\\\')\n",
    "\n",
    "# train_df = load_dataset(os.path.join(dataset_dir,\"aclImdb\", \"train\"))\n",
    "# test_df = load_dataset(os.path.join(dataset_dir,\"aclImdb\", \"test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# load\n",
    "with open('IMDb binary dataset/train_data.pickle', 'rb') as f:\n",
    "    train = pickle.load(f)\n",
    "    \n",
    "# load\n",
    "with open('IMDb binary dataset/test_data.pickle', 'rb') as f:\n",
    "    test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 설명\n",
    "Within these\n",
    "directories, reviews are stored in text files named following the\n",
    "convention [[id]_[rating].txt] where [id] is a unique id and [rating] is\n",
    "the star rating for that review on a 1-10 scale. For example, the file\n",
    "[test/pos/200_8.txt] is the text for a positive-labeled test set\n",
    "example with unique id 200 and star rating 8/10 from IMDb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Once again, Pia Zadora, the woman who owes her...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I felt like I was watching the Fast and the Fu...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I was so entertained throughout this insightfu...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This movie is really bad. The acting is plain ...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I only saw this movie once, and that was enoug...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence sentiment  polarity\n",
       "0  Once again, Pia Zadora, the woman who owes her...         1         0\n",
       "1  I felt like I was watching the Fast and the Fu...         3         0\n",
       "2  I was so entertained throughout this insightfu...         8         1\n",
       "3  This movie is really bad. The acting is plain ...         2         0\n",
       "4  I only saw this movie once, and that was enoug...         1         0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XA8WHJgzhIZf"
   },
   "source": [
    "To keep training fast, we'll take a sample of 5000 train and test examples, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lw_F488eixTV"
   },
   "outputs": [],
   "source": [
    "train = train.sample(5120)\n",
    "test = test.sample(5120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sfRnHSz3iSXz"
   },
   "source": [
    "For us, our input data is the 'sentence' column and our label is the 'polarity' column (0, 1 for negative and positive, respecitvely)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IuMOGwFui4it"
   },
   "outputs": [],
   "source": [
    "DATA_COLUMN = 'sentence'\n",
    "LABEL_COLUMN = 'polarity'\n",
    "# label_list is the list of labels, i.e. True, False or 0, 1 or 'dog', 'cat'\n",
    "label_list = [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets (Only take up to max_seq_length words for memory)\n",
    "train_text = train['sentence'].tolist()\n",
    "# MAX_SEQ_LENGTH만큼 자르고 합치기 \n",
    "train_text = [' '.join(t.split()[0:max_seq_length]) for t in train_text]\n",
    "## 1차원 리스트를 개별적으로 묶기. 2차원으로\n",
    "train_text = np.array(train_text, dtype=object)[:, np.newaxis]\n",
    "#y label..0 or 1\n",
    "train_label = train['polarity'].tolist()\n",
    "\n",
    "test_text = test['sentence'].tolist()\n",
    "test_text = [' '.join(t.split()[0:max_seq_length]) for t in test_text]\n",
    "test_text = np.array(test_text, dtype=object)[:, np.newaxis]\n",
    "test_label = test['polarity'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V399W0rqNJ-Z"
   },
   "source": [
    "We'll need to transform our data into a format BERT understands. This involves two steps. First, we create  `InputExample`'s using the constructor provided in the BERT library.\n",
    "\n",
    "- `text_a` is the text we want to classify, which in this case, is the `Request` field in our Dataframe. \n",
    "\n",
    "'text_a'는 우리가 분류하고싶은 텍스트\n",
    "\n",
    "- `text_b` is used if we're training a model to understand the relationship between sentences (i.e. is `text_b` a translation of `text_a`? Is `text_b` an answer to the question asked by `text_a`?). This doesn't apply to our task, so we can leave `text_b` blank.\n",
    "\n",
    "text_b는 text_a와의 관계성을 나타낼 때 쓰인다\n",
    "\n",
    "- `label` is the label for our example, i.e. True, False\n",
    "\n",
    "라벨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SCZWZtKxObjh"
   },
   "source": [
    "Next, we need to preprocess our data so that it matches the data BERT was trained on. For this, we'll need to do a couple of things (but don't worry--this is also included in the Python library):\n",
    "\n",
    "\n",
    "1. Lowercase our text (if we're using a BERT lowercase model)\n",
    "    - 텍스트 소문자화\n",
    "2. Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\n",
    "    - 텍스트 토큰화\n",
    "3. Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\n",
    "    - 단어 쪼개기\n",
    "4. Map our words to indexes using a vocab file that BERT provides\n",
    "    - BERT가 제공하는 vocab file을 이용해서 word에 index를 mapping\n",
    "5. Add special \"CLS\" and \"SEP\" tokens (see the [readme](https://github.com/google-research/bert))\n",
    "    - CLS, SEP 토큰 이용.\n",
    "6. Append \"index\" and \"segment\" tokens to each input (see the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf))\n",
    "    - 각 input에 Index, segment 토큰 추가\n",
    "\n",
    "Happily, we don't have to worry about most of these details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fake example\n",
    "\n",
    "class PaddingInputExample(object):\n",
    "    \"\"\"Fake example so the num input examples is a multiple of the batch size.\n",
    "  When running eval/predict on the TPU, we need to pad the number of examples\n",
    "  to be a multiple of the batch size, because the TPU requires a fixed batch\n",
    "  size. The alternative is to drop the last batch, which is bad because it means\n",
    "  the entire output data won't be generated.\n",
    "  We use this class instead of `None` because treating `None` as padding\n",
    "  battches could cause silent errors.\n",
    "  \"\"\"\n",
    "\n",
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "    Args:\n",
    "      guid: Unique id for the example.\n",
    "      text_a: string. The untokenized text of the first sequence. For single\n",
    "        sequence tasks, only this sequence must be specified.\n",
    "      text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "        Only must be specified for sequence pair tasks.\n",
    "      label: (Optional) string. The label of the example. This should be\n",
    "        specified for train and dev examples, but not for test examples.\n",
    "    \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0504 18:46:34.136799 10548 tf_logging.py:115] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['john', 'johan', '##son', \"'\", 's', 'house']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_tokenizer_from_hub_module():\n",
    "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "    bert_module =  hub.Module(bert_path)\n",
    "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "    vocab_file, do_lower_case = sess.run(\n",
    "        [\n",
    "            tokenization_info[\"vocab_file\"],\n",
    "            tokenization_info[\"do_lower_case\"],\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "\n",
    "# Instantiate tokenizer\n",
    "tokenizer = create_tokenizer_from_hub_module()\n",
    "tokenizer.tokenize(\"John Johanson 's   house\")\n",
    "# Input:  John Johanson 's   house\n",
    "#Tokens:  [\"John\", \"Johanson\", \"'s\",  \"house\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input, label\n",
    "# Append \"index\" and \"segment\" tokens to each input\n",
    "\n",
    "def convert_single_example(tokenizer, example, max_seq_length=256):\n",
    "    \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n",
    "    \n",
    "    # make fake example\n",
    "    if isinstance(example, PaddingInputExample):\n",
    "        input_ids = [0] * max_seq_length\n",
    "        input_mask = [0] * max_seq_length\n",
    "        segment_ids = [0] * max_seq_length\n",
    "        label = 0\n",
    "        return input_ids, input_mask, segment_ids, label\n",
    "\n",
    "    # tokenize the text we want to classify\n",
    "    tokens_a = tokenizer.tokenize(example.text_a)  \n",
    "    ## tokens_a:  [\"john\" ,\"johan\" ,\"##son\" ,\"'\" ,\"s\" ,\"house\"]\n",
    "    \n",
    "    # max_seq_length길이보다 길다면 앞에서부터 max_seq_length만큼 자르기\n",
    "    if len(tokens_a) > max_seq_length - 2:\n",
    "        tokens_a = tokens_a[0 : (max_seq_length - 2)]\n",
    "\n",
    "        \n",
    "    # Add special \"CLS\" and \"SEP\" tokens\n",
    "    # segment_ids : token + 2 개수만큼 0 인 리스트\n",
    "    tokens = []\n",
    "    segment_ids = []\n",
    "    tokens.append(\"[CLS]\")\n",
    "    segment_ids.append(0)\n",
    "    for token in tokens_a:\n",
    "        tokens.append(token)\n",
    "        segment_ids.append(0)\n",
    "    tokens.append(\"[SEP]\")\n",
    "    segment_ids.append(0)  \n",
    "    ## bert_tokens == [\"[CLS]\", \"john\", \"johan\", \"##son\", \"'\", \"s\", \"house\", \"[SEP]\"]\n",
    "\n",
    "    # convert tokens to id\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    ## [101, 2198, 13093, 3385, 1005, 1055, 2160, 102]\n",
    "    \n",
    "    \n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. \n",
    "    # Only real tokens are attended to.\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "    while len(input_ids) < max_seq_length:\n",
    "        input_ids.append(0)\n",
    "        input_mask.append(0)\n",
    "        segment_ids.append(0)\n",
    "\n",
    "    # 조건에 부합하지 않으면 error 발생시킴\n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "\n",
    "    return input_ids, input_mask, segment_ids, example.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(tokenizer, examples, max_seq_length=256):\n",
    "    \"\"\"Convert a set of `InputExample`s to a list of `InputFeatures`.\"\"\"\n",
    "\n",
    "    input_ids, input_masks, segment_ids, labels = [], [], [], []\n",
    "    for example in tqdm_notebook(examples, desc=\"Converting examples to features\"):\n",
    "        input_id, input_mask, segment_id, label = convert_single_example(\n",
    "            tokenizer, example, max_seq_length\n",
    "        )\n",
    "        input_ids.append(input_id)\n",
    "        input_masks.append(input_mask)\n",
    "        segment_ids.append(segment_id)\n",
    "        labels.append(label)\n",
    "    return (\n",
    "        np.array(input_ids),\n",
    "        np.array(input_masks),\n",
    "        np.array(segment_ids),\n",
    "        np.array(labels).reshape(-1, 1),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['\"Capitães de Abril\" is a very good. The story isn\\'t a documentary about the 1974 revolution in Portugal. But it gives us an idea of how it was like. The fiction of the story isn\\'t of great interest, but it doesn\\'t spoil the movie. The heroic actions of Captain Salgueiro Maia aren\\'t exaggerations and the film is also a tribute for his deeds. Captain Salgueiro Maia remains one of the greatest heroes of the 25th of April Revolution.<br /><br />All the actors are very good and even the smallest roles are played wonderfully. Lisbon looks beautiful as ever. Don\\'t miss it! I liked this film very much.'],\n",
       "       [\"Three horror stories based on members of a transgressive Hindu cult that return home but changed in some way. In the first story our former cult member is now in an insane asylum and is visited by a reported who wants to find out about what went on at the cult. Somewhat slow going as story is told in flashbacks while the two sit on chairs and face each other. Reporter is particularly interested in what lead to the death of the participants. What seemed rather boring suddenly turns very exciting with a surprising twist in the story. Things get quite bloody.<br /><br />Second story has a violent young criminal visiting a psychiatrist for mandatory therapy. The patient seems to have some type of agenda but the psychiatrist is up to the task. Again, things slow down a bit and get weird. Then there's a strange twist in the story that is very well written and surprising.<br /><br />Final story deals with spiritual healer who claims to be able to remove the persons illness from them with his hands. One of the patients is a former cult member, so the successful healing gets more complicated. Again, we are surprised by a twist. Has a pretty gory scene in there.<br /><br />There some nice female full frontal nudity as well as male full frontal nudity for some reason. I found the stories to be very well written and the director succeeds entirely in setting up each story with its surprising twist and the gory aftermath.<br /><br />Note:\"]],\n",
       "      dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[1, 1]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train_text[:2])\n",
    "display(train_label[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0504 18:46:36.893450 10548 tf_logging.py:115] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b94704df3204e66973fbf1b345d40e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Converting examples to features', max=5120, style=ProgressSty…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad87b79eb8d945da939a1b0f9f218a4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Converting examples to features', max=5120, style=ProgressSty…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def convert_text_to_examples(texts, labels):\n",
    "    \"\"\"Create InputExamples\"\"\"\n",
    "    InputExamples = []\n",
    "    for text, label in zip(texts, labels):\n",
    "        InputExamples.append(\n",
    "            InputExample(guid=None, text_a=\" \".join(text), text_b=None, label=label)\n",
    "        )\n",
    "    return InputExamples\n",
    "\n",
    "# Instantiate tokenizer\n",
    "tokenizer = create_tokenizer_from_hub_module()\n",
    "\n",
    "# Convert data to InputExample format\n",
    "train_examples = convert_text_to_examples(train_text, train_label)\n",
    "test_examples = convert_text_to_examples(test_text, test_label)\n",
    "\n",
    "# Convert to features\n",
    "(train_input_ids, train_input_masks, train_segment_ids, train_labels \n",
    ") = convert_examples_to_features(tokenizer, train_examples, max_seq_length=max_seq_length)\n",
    "(test_input_ids, test_input_masks, test_segment_ids, test_labels\n",
    ") = convert_examples_to_features(tokenizer, test_examples, max_seq_length=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_segment_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Example *** <br></br>\n",
    "guid: None<br><br></br></br>\n",
    "tokens: [CLS] i was over ##taken by the emotion . un ##for ##get ##table rendering of a wartime story which is unknown to most people . the performances were fault ##less and outstanding . [SEP]<br><br></br></br>\n",
    "input_ids: 101 1045 2001 2058 25310 2011 1996 7603 1012 4895 29278 18150 10880 14259 1997 1037 12498 2466 2029 2003 4242 2000 2087 2111 1012 1996 4616 2020 6346 3238 1998 5151 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0<br><br></br></br>\n",
    "input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0<br><br></br></br>\n",
    "segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0<br><br></br></br>\n",
    "label: 1 (id = 1)<br><br></br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a model\n",
    "\n",
    "Now that we've prepared our data, let's focus on building a model. `create_model` does just this below. \n",
    "\n",
    "First, it loads the BERT tf hub module again (this time to extract the computation graph). \n",
    "\n",
    "Next, it creates a single new layer that will be trained to adapt BERT to our sentiment task \n",
    "\n",
    "(i.e. classifying whether a movie review is positive or negative).\n",
    "\n",
    "This strategy of using a mostly trained model is called [fine-tuning](http://wiki.fast.ai/index.php/Fine_tuning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We next build a custom layer using Keras, integrating BERT from tf-hub. \n",
    "# The model is very large (110,302,011 parameters!!!) so we fine tune a subset of layers.\n",
    "\n",
    "class BertLayer(tf.layers.Layer):\n",
    "    def __init__(self, n_fine_tune_layers=10, **kwargs):\n",
    "        self.n_fine_tune_layers = n_fine_tune_layers\n",
    "        self.trainable = True\n",
    "        self.output_size = 768\n",
    "        super(BertLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.bert = hub.Module(\n",
    "            bert_path,\n",
    "            trainable=self.trainable,\n",
    "            name=\"{}_module\".format(self.name)\n",
    "        )\n",
    "\n",
    "        trainable_vars = self.bert.variables\n",
    "\n",
    "        # Remove unused layers\n",
    "        trainable_vars = [var for var in trainable_vars if not \"/cls/\" in var.name]\n",
    "\n",
    "        # Select how many layers to fine tune\n",
    "        trainable_vars = trainable_vars[-self.n_fine_tune_layers :]\n",
    "\n",
    "        # Add to trainable weights\n",
    "        for var in trainable_vars:\n",
    "            self._trainable_weights.append(var)\n",
    "            \n",
    "        for var in self.bert.variables:\n",
    "            if var not in self._trainable_weights:\n",
    "                self._non_trainable_weights.append(var)\n",
    "\n",
    "        super(BertLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
    "        input_ids, input_mask, segment_ids = inputs\n",
    "        bert_inputs = dict(\n",
    "            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n",
    "        )\n",
    "        result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "            \"pooled_output\"\n",
    "        ]\n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_model(max_seq_length): \n",
    "    in_id = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_ids\")\n",
    "    in_mask = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_masks\")\n",
    "    in_segment = tf.keras.layers.Input(shape=(max_seq_length,), name=\"segment_ids\")\n",
    "    bert_inputs = [in_id, in_mask, in_segment]\n",
    "    \n",
    "    bert_output = BertLayer(n_fine_tune_layers=3)(bert_inputs)\n",
    "    dense = tf.keras.layers.Dense(256, activation='relu')(bert_output)\n",
    "    pred = tf.keras.layers.Dense(1, activation='sigmoid')(dense)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=bert_inputs, outputs=pred)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def initialize_vars(sess):\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0504 18:47:05.245626 10548 tf_logging.py:115] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0504 18:47:06.637903 10548 tf_logging.py:115] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert_layer_1 (BertLayer)        (None, 768)          110104890   input_ids[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          196864      bert_layer_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            257         dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 110,302,011\n",
      "Trainable params: 3,147,009\n",
      "Non-trainable params: 107,155,002\n",
      "__________________________________________________________________________________________________\n",
      "Train on 5120 samples, validate on 5120 samples\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3264/5120 [==================>...........] - ETA: 20:33 - loss: 0.7052 - acc: 0.50 - ETA: 12:55 - loss: 1.5897 - acc: 0.46 - ETA: 10:23 - loss: 1.2803 - acc: 0.56 - ETA: 9:05 - loss: 1.2470 - acc: 0.5469 - ETA: 8:19 - loss: 1.1177 - acc: 0.587 - ETA: 7:47 - loss: 1.0416 - acc: 0.572 - ETA: 7:25 - loss: 0.9961 - acc: 0.544 - ETA: 7:07 - loss: 0.9575 - acc: 0.539 - ETA: 6:53 - loss: 0.9215 - acc: 0.541 - ETA: 6:42 - loss: 0.8926 - acc: 0.562 - ETA: 6:33 - loss: 0.8775 - acc: 0.562 - ETA: 6:25 - loss: 0.8615 - acc: 0.562 - ETA: 6:18 - loss: 0.8438 - acc: 0.567 - ETA: 6:12 - loss: 0.8375 - acc: 0.558 - ETA: 6:07 - loss: 0.8188 - acc: 0.562 - ETA: 6:02 - loss: 0.8038 - acc: 0.570 - ETA: 5:58 - loss: 0.7978 - acc: 0.566 - ETA: 5:54 - loss: 0.7906 - acc: 0.569 - ETA: 5:50 - loss: 0.7755 - acc: 0.582 - ETA: 5:47 - loss: 0.7708 - acc: 0.578 - ETA: 5:44 - loss: 0.7600 - acc: 0.586 - ETA: 5:41 - loss: 0.7590 - acc: 0.585 - ETA: 5:38 - loss: 0.7430 - acc: 0.595 - ETA: 5:35 - loss: 0.7365 - acc: 0.596 - ETA: 5:33 - loss: 0.7348 - acc: 0.595 - ETA: 5:31 - loss: 0.7210 - acc: 0.605 - ETA: 5:28 - loss: 0.7141 - acc: 0.613 - ETA: 5:26 - loss: 0.7221 - acc: 0.613 - ETA: 5:24 - loss: 0.7175 - acc: 0.618 - ETA: 5:22 - loss: 0.7096 - acc: 0.625 - ETA: 5:20 - loss: 0.7063 - acc: 0.629 - ETA: 5:18 - loss: 0.6941 - acc: 0.638 - ETA: 5:16 - loss: 0.6876 - acc: 0.645 - ETA: 5:14 - loss: 0.6769 - acc: 0.652 - ETA: 5:12 - loss: 0.6673 - acc: 0.658 - ETA: 5:11 - loss: 0.6567 - acc: 0.668 - ETA: 5:09 - loss: 0.6456 - acc: 0.675 - ETA: 5:07 - loss: 0.6393 - acc: 0.680 - ETA: 5:06 - loss: 0.6286 - acc: 0.685 - ETA: 5:04 - loss: 0.6222 - acc: 0.689 - ETA: 5:03 - loss: 0.6123 - acc: 0.692 - ETA: 5:01 - loss: 0.6070 - acc: 0.694 - ETA: 4:59 - loss: 0.6162 - acc: 0.694 - ETA: 4:58 - loss: 0.6039 - acc: 0.701 - ETA: 4:56 - loss: 0.5984 - acc: 0.705 - ETA: 4:55 - loss: 0.5932 - acc: 0.709 - ETA: 4:54 - loss: 0.5944 - acc: 0.711 - ETA: 4:52 - loss: 0.5875 - acc: 0.714 - ETA: 4:51 - loss: 0.5870 - acc: 0.713 - ETA: 4:49 - loss: 0.5825 - acc: 0.715 - ETA: 4:48 - loss: 0.5772 - acc: 0.719 - ETA: 4:47 - loss: 0.5721 - acc: 0.722 - ETA: 4:45 - loss: 0.5686 - acc: 0.721 - ETA: 4:44 - loss: 0.5649 - acc: 0.722 - ETA: 4:43 - loss: 0.5636 - acc: 0.725 - ETA: 4:41 - loss: 0.5594 - acc: 0.726 - ETA: 4:40 - loss: 0.5596 - acc: 0.725 - ETA: 4:39 - loss: 0.5578 - acc: 0.728 - ETA: 4:38 - loss: 0.5530 - acc: 0.730 - ETA: 4:36 - loss: 0.5499 - acc: 0.731 - ETA: 4:35 - loss: 0.5440 - acc: 0.734 - ETA: 4:34 - loss: 0.5437 - acc: 0.736 - ETA: 4:33 - loss: 0.5431 - acc: 0.738 - ETA: 4:31 - loss: 0.5412 - acc: 0.738 - ETA: 4:30 - loss: 0.5399 - acc: 0.739 - ETA: 4:29 - loss: 0.5392 - acc: 0.740 - ETA: 4:28 - loss: 0.5351 - acc: 0.743 - ETA: 4:26 - loss: 0.5348 - acc: 0.742 - ETA: 4:25 - loss: 0.5343 - acc: 0.741 - ETA: 4:24 - loss: 0.5293 - acc: 0.745 - ETA: 4:23 - loss: 0.5258 - acc: 0.748 - ETA: 4:22 - loss: 0.5302 - acc: 0.745 - ETA: 4:20 - loss: 0.5294 - acc: 0.746 - ETA: 4:19 - loss: 0.5295 - acc: 0.745 - ETA: 4:18 - loss: 0.5267 - acc: 0.746 - ETA: 4:17 - loss: 0.5262 - acc: 0.745 - ETA: 4:16 - loss: 0.5233 - acc: 0.746 - ETA: 4:15 - loss: 0.5226 - acc: 0.746 - ETA: 4:14 - loss: 0.5198 - acc: 0.749 - ETA: 4:12 - loss: 0.5157 - acc: 0.751 - ETA: 4:11 - loss: 0.5118 - acc: 0.753 - ETA: 4:10 - loss: 0.5079 - acc: 0.755 - ETA: 4:09 - loss: 0.5072 - acc: 0.755 - ETA: 4:08 - loss: 0.5068 - acc: 0.756 - ETA: 4:07 - loss: 0.5042 - acc: 0.758 - ETA: 4:06 - loss: 0.4991 - acc: 0.760 - ETA: 4:04 - loss: 0.4967 - acc: 0.761 - ETA: 4:03 - loss: 0.4940 - acc: 0.763 - ETA: 4:02 - loss: 0.4896 - acc: 0.766 - ETA: 4:01 - loss: 0.4890 - acc: 0.766 - ETA: 4:00 - loss: 0.4950 - acc: 0.765 - ETA: 3:59 - loss: 0.4928 - acc: 0.767 - ETA: 3:58 - loss: 0.4883 - acc: 0.769 - ETA: 3:57 - loss: 0.4938 - acc: 0.768 - ETA: 3:55 - loss: 0.4933 - acc: 0.768 - ETA: 3:54 - loss: 0.4895 - acc: 0.770 - ETA: 3:53 - loss: 0.4864 - acc: 0.771 - ETA: 3:52 - loss: 0.4856 - acc: 0.772 - ETA: 3:51 - loss: 0.4840 - acc: 0.772 - ETA: 3:50 - loss: 0.4839 - acc: 0.773 - ETA: 3:49 - loss: 0.4810 - acc: 0.774 - ETA: 3:48 - loss: 0.4810 - acc: 0.774 - ETA: 3:46 - loss: 0.4801 - acc: 0.774 - ETA: 3:45 - loss: 0.4769 - acc: 0.776 - ETA: 3:44 - loss: 0.4747 - acc: 0.777 - ETA: 3:43 - loss: 0.4739 - acc: 0.777 - ETA: 3:42 - loss: 0.4726 - acc: 0.778 - ETA: 3:41 - loss: 0.4736 - acc: 0.778 - ETA: 3:40 - loss: 0.4723 - acc: 0.778 - ETA: 3:39 - loss: 0.4708 - acc: 0.779 - ETA: 3:38 - loss: 0.4698 - acc: 0.779 - ETA: 3:37 - loss: 0.4692 - acc: 0.780 - ETA: 3:36 - loss: 0.4680 - acc: 0.781 - ETA: 3:34 - loss: 0.4686 - acc: 0.781 - ETA: 3:33 - loss: 0.4678 - acc: 0.781 - ETA: 3:32 - loss: 0.4670 - acc: 0.781 - ETA: 3:31 - loss: 0.4658 - acc: 0.782 - ETA: 3:30 - loss: 0.4640 - acc: 0.783 - ETA: 3:29 - loss: 0.4637 - acc: 0.783 - ETA: 3:28 - loss: 0.4619 - acc: 0.783 - ETA: 3:27 - loss: 0.4618 - acc: 0.783 - ETA: 3:26 - loss: 0.4598 - acc: 0.784 - ETA: 3:25 - loss: 0.4574 - acc: 0.786 - ETA: 3:24 - loss: 0.4574 - acc: 0.786 - ETA: 3:23 - loss: 0.4554 - acc: 0.788 - ETA: 3:21 - loss: 0.4540 - acc: 0.789 - ETA: 3:20 - loss: 0.4523 - acc: 0.789 - ETA: 3:19 - loss: 0.4494 - acc: 0.791 - ETA: 3:18 - loss: 0.4497 - acc: 0.791 - ETA: 3:17 - loss: 0.4485 - acc: 0.792 - ETA: 3:16 - loss: 0.4462 - acc: 0.793 - ETA: 3:15 - loss: 0.4448 - acc: 0.794 - ETA: 3:14 - loss: 0.4442 - acc: 0.794 - ETA: 3:13 - loss: 0.4438 - acc: 0.795 - ETA: 3:12 - loss: 0.4417 - acc: 0.796 - ETA: 3:11 - loss: 0.4403 - acc: 0.796 - ETA: 3:10 - loss: 0.4395 - acc: 0.797 - ETA: 3:09 - loss: 0.4370 - acc: 0.799 - ETA: 3:08 - loss: 0.4361 - acc: 0.800 - ETA: 3:07 - loss: 0.4347 - acc: 0.800 - ETA: 3:06 - loss: 0.4341 - acc: 0.801 - ETA: 3:05 - loss: 0.4331 - acc: 0.802 - ETA: 3:03 - loss: 0.4321 - acc: 0.802 - ETA: 3:02 - loss: 0.4312 - acc: 0.803 - ETA: 3:01 - loss: 0.4300 - acc: 0.804 - ETA: 3:00 - loss: 0.4288 - acc: 0.804 - ETA: 2:59 - loss: 0.4279 - acc: 0.805 - ETA: 2:58 - loss: 0.4260 - acc: 0.806 - ETA: 2:57 - loss: 0.4251 - acc: 0.807 - ETA: 2:56 - loss: 0.4270 - acc: 0.806 - ETA: 2:55 - loss: 0.4264 - acc: 0.806 - ETA: 2:54 - loss: 0.4259 - acc: 0.806 - ETA: 2:53 - loss: 0.4253 - acc: 0.807 - ETA: 2:52 - loss: 0.4246 - acc: 0.807 - ETA: 2:51 - loss: 0.4246 - acc: 0.808 - ETA: 2:50 - loss: 0.4239 - acc: 0.808 - ETA: 2:49 - loss: 0.4222 - acc: 0.809 - ETA: 2:48 - loss: 0.4220 - acc: 0.809 - ETA: 2:47 - loss: 0.4207 - acc: 0.810 - ETA: 2:45 - loss: 0.4193 - acc: 0.811 - ETA: 2:44 - loss: 0.4176 - acc: 0.812 - ETA: 2:43 - loss: 0.4167 - acc: 0.812 - ETA: 2:42 - loss: 0.4153 - acc: 0.813 - ETA: 2:41 - loss: 0.4131 - acc: 0.814 - ETA: 2:40 - loss: 0.4135 - acc: 0.814 - ETA: 2:39 - loss: 0.4137 - acc: 0.814 - ETA: 2:38 - loss: 0.4128 - acc: 0.815 - ETA: 2:37 - loss: 0.4120 - acc: 0.815 - ETA: 2:36 - loss: 0.4116 - acc: 0.815 - ETA: 2:35 - loss: 0.4096 - acc: 0.816 - ETA: 2:34 - loss: 0.4084 - acc: 0.817 - ETA: 2:33 - loss: 0.4080 - acc: 0.817 - ETA: 2:32 - loss: 0.4066 - acc: 0.817 - ETA: 2:31 - loss: 0.4070 - acc: 0.817 - ETA: 2:30 - loss: 0.4054 - acc: 0.818 - ETA: 2:29 - loss: 0.4047 - acc: 0.818 - ETA: 2:28 - loss: 0.4036 - acc: 0.819 - ETA: 2:27 - loss: 0.4068 - acc: 0.817 - ETA: 2:25 - loss: 0.4052 - acc: 0.818 - ETA: 2:24 - loss: 0.4055 - acc: 0.818 - ETA: 2:23 - loss: 0.4051 - acc: 0.818 - ETA: 2:22 - loss: 0.4050 - acc: 0.818 - ETA: 2:21 - loss: 0.4048 - acc: 0.818 - ETA: 2:20 - loss: 0.4054 - acc: 0.817 - ETA: 2:19 - loss: 0.4051 - acc: 0.817 - ETA: 2:18 - loss: 0.4056 - acc: 0.817 - ETA: 2:17 - loss: 0.4050 - acc: 0.817 - ETA: 2:16 - loss: 0.4041 - acc: 0.817 - ETA: 2:15 - loss: 0.4031 - acc: 0.818 - ETA: 2:14 - loss: 0.4027 - acc: 0.818 - ETA: 2:13 - loss: 0.4016 - acc: 0.818 - ETA: 2:12 - loss: 0.4027 - acc: 0.818 - ETA: 2:11 - loss: 0.4016 - acc: 0.818 - ETA: 2:10 - loss: 0.4027 - acc: 0.818 - ETA: 2:09 - loss: 0.4029 - acc: 0.817 - ETA: 2:08 - loss: 0.4032 - acc: 0.818 - ETA: 2:07 - loss: 0.4023 - acc: 0.818 - ETA: 2:06 - loss: 0.4007 - acc: 0.819 - ETA: 2:05 - loss: 0.3995 - acc: 0.820 - ETA: 2:04 - loss: 0.3996 - acc: 0.820 - ETA: 2:03 - loss: 0.3986 - acc: 0.820 - ETA: 2:01 - loss: 0.4002 - acc: 0.820 - ETA: 2:00 - loss: 0.4009 - acc: 0.820 - ETA: 1:59 - loss: 0.3998 - acc: 0.8208"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5120/5120 [==============================] - ETA: 1:58 - loss: 0.3994 - acc: 0.821 - ETA: 1:57 - loss: 0.3991 - acc: 0.821 - ETA: 1:56 - loss: 0.3984 - acc: 0.821 - ETA: 1:55 - loss: 0.3974 - acc: 0.821 - ETA: 1:54 - loss: 0.3965 - acc: 0.821 - ETA: 1:53 - loss: 0.3960 - acc: 0.822 - ETA: 1:52 - loss: 0.3959 - acc: 0.821 - ETA: 1:51 - loss: 0.3965 - acc: 0.821 - ETA: 1:50 - loss: 0.3949 - acc: 0.822 - ETA: 1:49 - loss: 0.3951 - acc: 0.822 - ETA: 1:48 - loss: 0.3957 - acc: 0.821 - ETA: 1:47 - loss: 0.3961 - acc: 0.821 - ETA: 1:46 - loss: 0.3953 - acc: 0.822 - ETA: 1:45 - loss: 0.3963 - acc: 0.821 - ETA: 1:44 - loss: 0.3958 - acc: 0.821 - ETA: 1:43 - loss: 0.3945 - acc: 0.822 - ETA: 1:42 - loss: 0.3936 - acc: 0.823 - ETA: 1:41 - loss: 0.3932 - acc: 0.823 - ETA: 1:40 - loss: 0.3931 - acc: 0.823 - ETA: 1:39 - loss: 0.3926 - acc: 0.823 - ETA: 1:38 - loss: 0.3925 - acc: 0.823 - ETA: 1:37 - loss: 0.3931 - acc: 0.823 - ETA: 1:36 - loss: 0.3925 - acc: 0.823 - ETA: 1:34 - loss: 0.3934 - acc: 0.822 - ETA: 1:33 - loss: 0.3927 - acc: 0.823 - ETA: 1:32 - loss: 0.3938 - acc: 0.823 - ETA: 1:31 - loss: 0.3930 - acc: 0.823 - ETA: 1:30 - loss: 0.3917 - acc: 0.824 - ETA: 1:29 - loss: 0.3912 - acc: 0.824 - ETA: 1:28 - loss: 0.3914 - acc: 0.824 - ETA: 1:27 - loss: 0.3918 - acc: 0.823 - ETA: 1:26 - loss: 0.3914 - acc: 0.823 - ETA: 1:25 - loss: 0.3912 - acc: 0.824 - ETA: 1:24 - loss: 0.3905 - acc: 0.824 - ETA: 1:23 - loss: 0.3906 - acc: 0.824 - ETA: 1:22 - loss: 0.3898 - acc: 0.825 - ETA: 1:21 - loss: 0.3890 - acc: 0.825 - ETA: 1:20 - loss: 0.3889 - acc: 0.825 - ETA: 1:19 - loss: 0.3895 - acc: 0.825 - ETA: 1:18 - loss: 0.3887 - acc: 0.825 - ETA: 1:17 - loss: 0.3878 - acc: 0.826 - ETA: 1:16 - loss: 0.3873 - acc: 0.826 - ETA: 1:15 - loss: 0.3873 - acc: 0.826 - ETA: 1:14 - loss: 0.3866 - acc: 0.826 - ETA: 1:13 - loss: 0.3857 - acc: 0.827 - ETA: 1:12 - loss: 0.3861 - acc: 0.826 - ETA: 1:11 - loss: 0.3871 - acc: 0.826 - ETA: 1:10 - loss: 0.3871 - acc: 0.827 - ETA: 1:09 - loss: 0.3863 - acc: 0.827 - ETA: 1:08 - loss: 0.3862 - acc: 0.827 - ETA: 1:07 - loss: 0.3858 - acc: 0.827 - ETA: 1:05 - loss: 0.3855 - acc: 0.827 - ETA: 1:04 - loss: 0.3843 - acc: 0.828 - ETA: 1:03 - loss: 0.3838 - acc: 0.828 - ETA: 1:02 - loss: 0.3840 - acc: 0.828 - ETA: 1:01 - loss: 0.3839 - acc: 0.829 - ETA: 1:00 - loss: 0.3833 - acc: 0.829 - ETA: 59s - loss: 0.3832 - acc: 0.829 - ETA: 58s - loss: 0.3838 - acc: 0.82 - ETA: 57s - loss: 0.3833 - acc: 0.82 - ETA: 56s - loss: 0.3825 - acc: 0.82 - ETA: 55s - loss: 0.3823 - acc: 0.82 - ETA: 54s - loss: 0.3816 - acc: 0.83 - ETA: 53s - loss: 0.3807 - acc: 0.83 - ETA: 52s - loss: 0.3807 - acc: 0.83 - ETA: 51s - loss: 0.3804 - acc: 0.83 - ETA: 50s - loss: 0.3797 - acc: 0.83 - ETA: 49s - loss: 0.3799 - acc: 0.83 - ETA: 48s - loss: 0.3794 - acc: 0.83 - ETA: 47s - loss: 0.3789 - acc: 0.83 - ETA: 46s - loss: 0.3780 - acc: 0.83 - ETA: 45s - loss: 0.3792 - acc: 0.83 - ETA: 44s - loss: 0.3791 - acc: 0.83 - ETA: 43s - loss: 0.3786 - acc: 0.83 - ETA: 42s - loss: 0.3780 - acc: 0.83 - ETA: 41s - loss: 0.3777 - acc: 0.83 - ETA: 40s - loss: 0.3778 - acc: 0.83 - ETA: 39s - loss: 0.3772 - acc: 0.83 - ETA: 38s - loss: 0.3768 - acc: 0.83 - ETA: 37s - loss: 0.3759 - acc: 0.83 - ETA: 36s - loss: 0.3767 - acc: 0.83 - ETA: 35s - loss: 0.3758 - acc: 0.83 - ETA: 34s - loss: 0.3775 - acc: 0.83 - ETA: 32s - loss: 0.3779 - acc: 0.83 - ETA: 31s - loss: 0.3777 - acc: 0.83 - ETA: 30s - loss: 0.3773 - acc: 0.83 - ETA: 29s - loss: 0.3768 - acc: 0.83 - ETA: 28s - loss: 0.3762 - acc: 0.83 - ETA: 27s - loss: 0.3760 - acc: 0.83 - ETA: 26s - loss: 0.3754 - acc: 0.83 - ETA: 25s - loss: 0.3745 - acc: 0.83 - ETA: 24s - loss: 0.3741 - acc: 0.83 - ETA: 23s - loss: 0.3744 - acc: 0.83 - ETA: 22s - loss: 0.3737 - acc: 0.83 - ETA: 21s - loss: 0.3734 - acc: 0.83 - ETA: 20s - loss: 0.3722 - acc: 0.83 - ETA: 19s - loss: 0.3714 - acc: 0.83 - ETA: 18s - loss: 0.3704 - acc: 0.83 - ETA: 17s - loss: 0.3697 - acc: 0.83 - ETA: 16s - loss: 0.3692 - acc: 0.83 - ETA: 15s - loss: 0.3706 - acc: 0.83 - ETA: 14s - loss: 0.3716 - acc: 0.83 - ETA: 13s - loss: 0.3705 - acc: 0.83 - ETA: 12s - loss: 0.3702 - acc: 0.83 - ETA: 11s - loss: 0.3697 - acc: 0.83 - ETA: 10s - loss: 0.3703 - acc: 0.83 - ETA: 9s - loss: 0.3714 - acc: 0.8366 - ETA: 8s - loss: 0.3717 - acc: 0.836 - ETA: 7s - loss: 0.3723 - acc: 0.836 - ETA: 6s - loss: 0.3720 - acc: 0.836 - ETA: 5s - loss: 0.3719 - acc: 0.836 - ETA: 4s - loss: 0.3729 - acc: 0.836 - ETA: 3s - loss: 0.3741 - acc: 0.835 - ETA: 2s - loss: 0.3741 - acc: 0.835 - ETA: 1s - loss: 0.3742 - acc: 0.835 - 669s 131ms/step - loss: 0.3737 - acc: 0.8357 - val_loss: 0.3800 - val_acc: 0.8316\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3264/5120 [==================>...........] - ETA: 5:24 - loss: 0.3027 - acc: 0.875 - ETA: 5:23 - loss: 0.2150 - acc: 0.937 - ETA: 5:23 - loss: 0.2693 - acc: 0.875 - ETA: 5:24 - loss: 0.2454 - acc: 0.890 - ETA: 5:22 - loss: 0.2217 - acc: 0.912 - ETA: 5:23 - loss: 0.2408 - acc: 0.906 - ETA: 5:23 - loss: 0.2439 - acc: 0.910 - ETA: 5:22 - loss: 0.2226 - acc: 0.914 - ETA: 5:21 - loss: 0.2220 - acc: 0.909 - ETA: 5:19 - loss: 0.2338 - acc: 0.906 - ETA: 5:18 - loss: 0.2735 - acc: 0.892 - ETA: 5:17 - loss: 0.2614 - acc: 0.895 - ETA: 5:16 - loss: 0.2950 - acc: 0.889 - ETA: 5:15 - loss: 0.2782 - acc: 0.892 - ETA: 5:13 - loss: 0.2785 - acc: 0.891 - ETA: 5:12 - loss: 0.2736 - acc: 0.890 - ETA: 5:11 - loss: 0.2666 - acc: 0.893 - ETA: 5:10 - loss: 0.2752 - acc: 0.888 - ETA: 5:09 - loss: 0.2633 - acc: 0.894 - ETA: 5:08 - loss: 0.2602 - acc: 0.896 - ETA: 5:07 - loss: 0.2606 - acc: 0.898 - ETA: 5:06 - loss: 0.2718 - acc: 0.892 - ETA: 5:05 - loss: 0.2700 - acc: 0.894 - ETA: 5:04 - loss: 0.2771 - acc: 0.890 - ETA: 5:02 - loss: 0.2738 - acc: 0.892 - ETA: 5:01 - loss: 0.2803 - acc: 0.889 - ETA: 5:00 - loss: 0.2825 - acc: 0.888 - ETA: 4:59 - loss: 0.2840 - acc: 0.888 - ETA: 4:58 - loss: 0.2797 - acc: 0.890 - ETA: 4:57 - loss: 0.2824 - acc: 0.885 - ETA: 4:56 - loss: 0.2783 - acc: 0.889 - ETA: 4:55 - loss: 0.2754 - acc: 0.888 - ETA: 4:54 - loss: 0.2715 - acc: 0.892 - ETA: 4:53 - loss: 0.2675 - acc: 0.895 - ETA: 4:51 - loss: 0.2689 - acc: 0.894 - ETA: 4:50 - loss: 0.2687 - acc: 0.892 - ETA: 4:49 - loss: 0.2681 - acc: 0.891 - ETA: 4:48 - loss: 0.2619 - acc: 0.894 - ETA: 4:47 - loss: 0.2649 - acc: 0.894 - ETA: 4:46 - loss: 0.2658 - acc: 0.893 - ETA: 4:45 - loss: 0.2617 - acc: 0.896 - ETA: 4:44 - loss: 0.2588 - acc: 0.897 - ETA: 4:43 - loss: 0.2590 - acc: 0.896 - ETA: 4:42 - loss: 0.2589 - acc: 0.894 - ETA: 4:41 - loss: 0.2536 - acc: 0.897 - ETA: 4:40 - loss: 0.2544 - acc: 0.896 - ETA: 4:39 - loss: 0.2682 - acc: 0.894 - ETA: 4:38 - loss: 0.2700 - acc: 0.893 - ETA: 4:37 - loss: 0.2692 - acc: 0.891 - ETA: 4:36 - loss: 0.2689 - acc: 0.891 - ETA: 4:35 - loss: 0.2706 - acc: 0.890 - ETA: 4:34 - loss: 0.2708 - acc: 0.889 - ETA: 4:33 - loss: 0.2722 - acc: 0.888 - ETA: 4:32 - loss: 0.2729 - acc: 0.888 - ETA: 4:31 - loss: 0.2701 - acc: 0.890 - ETA: 4:30 - loss: 0.2697 - acc: 0.890 - ETA: 4:29 - loss: 0.2698 - acc: 0.890 - ETA: 4:28 - loss: 0.2690 - acc: 0.891 - ETA: 4:27 - loss: 0.2668 - acc: 0.891 - ETA: 4:26 - loss: 0.2686 - acc: 0.891 - ETA: 4:25 - loss: 0.2698 - acc: 0.891 - ETA: 4:24 - loss: 0.2705 - acc: 0.891 - ETA: 4:23 - loss: 0.2684 - acc: 0.891 - ETA: 4:22 - loss: 0.2695 - acc: 0.891 - ETA: 4:21 - loss: 0.2681 - acc: 0.891 - ETA: 4:20 - loss: 0.2755 - acc: 0.888 - ETA: 4:18 - loss: 0.2872 - acc: 0.884 - ETA: 4:17 - loss: 0.2861 - acc: 0.885 - ETA: 4:16 - loss: 0.2849 - acc: 0.885 - ETA: 4:15 - loss: 0.2836 - acc: 0.886 - ETA: 4:14 - loss: 0.2836 - acc: 0.885 - ETA: 4:13 - loss: 0.2834 - acc: 0.886 - ETA: 4:12 - loss: 0.2845 - acc: 0.886 - ETA: 4:11 - loss: 0.2860 - acc: 0.885 - ETA: 4:10 - loss: 0.2867 - acc: 0.882 - ETA: 4:09 - loss: 0.2857 - acc: 0.883 - ETA: 4:08 - loss: 0.2863 - acc: 0.883 - ETA: 4:07 - loss: 0.2891 - acc: 0.882 - ETA: 4:06 - loss: 0.2909 - acc: 0.880 - ETA: 4:05 - loss: 0.2916 - acc: 0.879 - ETA: 4:04 - loss: 0.2931 - acc: 0.879 - ETA: 4:03 - loss: 0.2952 - acc: 0.878 - ETA: 4:02 - loss: 0.2959 - acc: 0.878 - ETA: 4:01 - loss: 0.2947 - acc: 0.878 - ETA: 4:00 - loss: 0.2957 - acc: 0.877 - ETA: 3:59 - loss: 0.2930 - acc: 0.878 - ETA: 3:58 - loss: 0.2944 - acc: 0.877 - ETA: 3:57 - loss: 0.2950 - acc: 0.877 - ETA: 3:56 - loss: 0.2924 - acc: 0.878 - ETA: 3:55 - loss: 0.2915 - acc: 0.878 - ETA: 3:54 - loss: 0.2916 - acc: 0.877 - ETA: 3:53 - loss: 0.2903 - acc: 0.878 - ETA: 3:52 - loss: 0.2918 - acc: 0.878 - ETA: 3:51 - loss: 0.2902 - acc: 0.879 - ETA: 3:50 - loss: 0.2914 - acc: 0.878 - ETA: 3:49 - loss: 0.2891 - acc: 0.880 - ETA: 3:48 - loss: 0.2898 - acc: 0.879 - ETA: 3:47 - loss: 0.2881 - acc: 0.880 - ETA: 3:46 - loss: 0.2863 - acc: 0.880 - ETA: 3:45 - loss: 0.2902 - acc: 0.878 - ETA: 3:44 - loss: 0.2891 - acc: 0.879 - ETA: 3:43 - loss: 0.2868 - acc: 0.880 - ETA: 3:41 - loss: 0.2844 - acc: 0.881 - ETA: 3:40 - loss: 0.2833 - acc: 0.881 - ETA: 3:39 - loss: 0.2834 - acc: 0.881 - ETA: 3:38 - loss: 0.2821 - acc: 0.881 - ETA: 3:37 - loss: 0.2818 - acc: 0.882 - ETA: 3:36 - loss: 0.2814 - acc: 0.881 - ETA: 3:35 - loss: 0.2800 - acc: 0.882 - ETA: 3:34 - loss: 0.2791 - acc: 0.883 - ETA: 3:33 - loss: 0.2800 - acc: 0.882 - ETA: 3:32 - loss: 0.2791 - acc: 0.882 - ETA: 3:31 - loss: 0.2806 - acc: 0.881 - ETA: 3:30 - loss: 0.2800 - acc: 0.882 - ETA: 3:29 - loss: 0.2838 - acc: 0.880 - ETA: 3:28 - loss: 0.2827 - acc: 0.880 - ETA: 3:27 - loss: 0.2820 - acc: 0.880 - ETA: 3:26 - loss: 0.2812 - acc: 0.880 - ETA: 3:25 - loss: 0.2808 - acc: 0.880 - ETA: 3:24 - loss: 0.2813 - acc: 0.880 - ETA: 3:23 - loss: 0.2803 - acc: 0.880 - ETA: 3:22 - loss: 0.2788 - acc: 0.881 - ETA: 3:21 - loss: 0.2780 - acc: 0.881 - ETA: 3:20 - loss: 0.2776 - acc: 0.882 - ETA: 3:19 - loss: 0.2778 - acc: 0.882 - ETA: 3:18 - loss: 0.2793 - acc: 0.881 - ETA: 3:17 - loss: 0.2786 - acc: 0.881 - ETA: 3:16 - loss: 0.2784 - acc: 0.881 - ETA: 3:15 - loss: 0.2771 - acc: 0.882 - ETA: 3:14 - loss: 0.2763 - acc: 0.882 - ETA: 3:13 - loss: 0.2763 - acc: 0.881 - ETA: 3:12 - loss: 0.2756 - acc: 0.881 - ETA: 3:11 - loss: 0.2749 - acc: 0.882 - ETA: 3:10 - loss: 0.2743 - acc: 0.882 - ETA: 3:09 - loss: 0.2736 - acc: 0.882 - ETA: 3:08 - loss: 0.2726 - acc: 0.882 - ETA: 3:07 - loss: 0.2734 - acc: 0.882 - ETA: 3:06 - loss: 0.2755 - acc: 0.882 - ETA: 3:05 - loss: 0.2818 - acc: 0.880 - ETA: 3:04 - loss: 0.2811 - acc: 0.880 - ETA: 3:03 - loss: 0.2802 - acc: 0.881 - ETA: 3:02 - loss: 0.2807 - acc: 0.881 - ETA: 3:01 - loss: 0.2796 - acc: 0.882 - ETA: 2:59 - loss: 0.2804 - acc: 0.881 - ETA: 2:58 - loss: 0.2828 - acc: 0.880 - ETA: 2:57 - loss: 0.2833 - acc: 0.879 - ETA: 2:56 - loss: 0.2829 - acc: 0.880 - ETA: 2:55 - loss: 0.2825 - acc: 0.880 - ETA: 2:54 - loss: 0.2816 - acc: 0.880 - ETA: 2:53 - loss: 0.2829 - acc: 0.880 - ETA: 2:52 - loss: 0.2818 - acc: 0.880 - ETA: 2:51 - loss: 0.2807 - acc: 0.881 - ETA: 2:50 - loss: 0.2817 - acc: 0.880 - ETA: 2:49 - loss: 0.2805 - acc: 0.881 - ETA: 2:48 - loss: 0.2800 - acc: 0.881 - ETA: 2:47 - loss: 0.2799 - acc: 0.881 - ETA: 2:46 - loss: 0.2786 - acc: 0.882 - ETA: 2:45 - loss: 0.2790 - acc: 0.882 - ETA: 2:44 - loss: 0.2779 - acc: 0.882 - ETA: 2:43 - loss: 0.2782 - acc: 0.882 - ETA: 2:42 - loss: 0.2777 - acc: 0.882 - ETA: 2:41 - loss: 0.2768 - acc: 0.883 - ETA: 2:40 - loss: 0.2775 - acc: 0.881 - ETA: 2:39 - loss: 0.2772 - acc: 0.882 - ETA: 2:38 - loss: 0.2764 - acc: 0.882 - ETA: 2:37 - loss: 0.2757 - acc: 0.882 - ETA: 2:36 - loss: 0.2755 - acc: 0.882 - ETA: 2:35 - loss: 0.2753 - acc: 0.882 - ETA: 2:34 - loss: 0.2745 - acc: 0.883 - ETA: 2:33 - loss: 0.2739 - acc: 0.883 - ETA: 2:32 - loss: 0.2745 - acc: 0.883 - ETA: 2:31 - loss: 0.2764 - acc: 0.881 - ETA: 2:30 - loss: 0.2759 - acc: 0.881 - ETA: 2:29 - loss: 0.2755 - acc: 0.882 - ETA: 2:28 - loss: 0.2749 - acc: 0.882 - ETA: 2:27 - loss: 0.2754 - acc: 0.881 - ETA: 2:26 - loss: 0.2757 - acc: 0.881 - ETA: 2:25 - loss: 0.2754 - acc: 0.881 - ETA: 2:24 - loss: 0.2750 - acc: 0.880 - ETA: 2:23 - loss: 0.2744 - acc: 0.881 - ETA: 2:22 - loss: 0.2737 - acc: 0.881 - ETA: 2:21 - loss: 0.2737 - acc: 0.881 - ETA: 2:20 - loss: 0.2724 - acc: 0.881 - ETA: 2:19 - loss: 0.2721 - acc: 0.881 - ETA: 2:18 - loss: 0.2714 - acc: 0.882 - ETA: 2:17 - loss: 0.2705 - acc: 0.882 - ETA: 2:15 - loss: 0.2719 - acc: 0.882 - ETA: 2:14 - loss: 0.2708 - acc: 0.882 - ETA: 2:13 - loss: 0.2708 - acc: 0.882 - ETA: 2:12 - loss: 0.2720 - acc: 0.882 - ETA: 2:11 - loss: 0.2711 - acc: 0.883 - ETA: 2:10 - loss: 0.2711 - acc: 0.883 - ETA: 2:09 - loss: 0.2711 - acc: 0.883 - ETA: 2:08 - loss: 0.2727 - acc: 0.883 - ETA: 2:07 - loss: 0.2728 - acc: 0.883 - ETA: 2:06 - loss: 0.2723 - acc: 0.883 - ETA: 2:05 - loss: 0.2719 - acc: 0.883 - ETA: 2:04 - loss: 0.2719 - acc: 0.883 - ETA: 2:03 - loss: 0.2728 - acc: 0.882 - ETA: 2:02 - loss: 0.2732 - acc: 0.882 - ETA: 2:01 - loss: 0.2729 - acc: 0.882 - ETA: 2:00 - loss: 0.2735 - acc: 0.882 - ETA: 1:59 - loss: 0.2755 - acc: 0.881 - ETA: 1:58 - loss: 0.2763 - acc: 0.8811"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5120/5120 [==============================] - ETA: 1:57 - loss: 0.2757 - acc: 0.881 - ETA: 1:56 - loss: 0.2771 - acc: 0.881 - ETA: 1:55 - loss: 0.2769 - acc: 0.881 - ETA: 1:54 - loss: 0.2768 - acc: 0.881 - ETA: 1:53 - loss: 0.2765 - acc: 0.881 - ETA: 1:52 - loss: 0.2755 - acc: 0.881 - ETA: 1:51 - loss: 0.2748 - acc: 0.882 - ETA: 1:50 - loss: 0.2754 - acc: 0.881 - ETA: 1:49 - loss: 0.2757 - acc: 0.881 - ETA: 1:48 - loss: 0.2754 - acc: 0.881 - ETA: 1:47 - loss: 0.2754 - acc: 0.881 - ETA: 1:46 - loss: 0.2745 - acc: 0.881 - ETA: 1:45 - loss: 0.2737 - acc: 0.882 - ETA: 1:44 - loss: 0.2732 - acc: 0.882 - ETA: 1:43 - loss: 0.2728 - acc: 0.882 - ETA: 1:42 - loss: 0.2718 - acc: 0.883 - ETA: 1:41 - loss: 0.2723 - acc: 0.882 - ETA: 1:40 - loss: 0.2715 - acc: 0.882 - ETA: 1:39 - loss: 0.2712 - acc: 0.882 - ETA: 1:38 - loss: 0.2711 - acc: 0.883 - ETA: 1:37 - loss: 0.2732 - acc: 0.882 - ETA: 1:36 - loss: 0.2726 - acc: 0.883 - ETA: 1:35 - loss: 0.2717 - acc: 0.883 - ETA: 1:34 - loss: 0.2706 - acc: 0.884 - ETA: 1:33 - loss: 0.2712 - acc: 0.883 - ETA: 1:31 - loss: 0.2706 - acc: 0.884 - ETA: 1:30 - loss: 0.2696 - acc: 0.884 - ETA: 1:29 - loss: 0.2686 - acc: 0.885 - ETA: 1:28 - loss: 0.2679 - acc: 0.885 - ETA: 1:27 - loss: 0.2691 - acc: 0.885 - ETA: 1:26 - loss: 0.2682 - acc: 0.885 - ETA: 1:25 - loss: 0.2676 - acc: 0.885 - ETA: 1:24 - loss: 0.2683 - acc: 0.885 - ETA: 1:23 - loss: 0.2693 - acc: 0.885 - ETA: 1:22 - loss: 0.2697 - acc: 0.885 - ETA: 1:21 - loss: 0.2694 - acc: 0.885 - ETA: 1:20 - loss: 0.2692 - acc: 0.885 - ETA: 1:19 - loss: 0.2695 - acc: 0.885 - ETA: 1:18 - loss: 0.2689 - acc: 0.886 - ETA: 1:17 - loss: 0.2688 - acc: 0.886 - ETA: 1:16 - loss: 0.2684 - acc: 0.886 - ETA: 1:15 - loss: 0.2688 - acc: 0.886 - ETA: 1:14 - loss: 0.2682 - acc: 0.886 - ETA: 1:13 - loss: 0.2691 - acc: 0.886 - ETA: 1:12 - loss: 0.2699 - acc: 0.886 - ETA: 1:11 - loss: 0.2690 - acc: 0.886 - ETA: 1:10 - loss: 0.2691 - acc: 0.886 - ETA: 1:09 - loss: 0.2692 - acc: 0.886 - ETA: 1:08 - loss: 0.2708 - acc: 0.886 - ETA: 1:07 - loss: 0.2702 - acc: 0.886 - ETA: 1:06 - loss: 0.2725 - acc: 0.885 - ETA: 1:05 - loss: 0.2728 - acc: 0.885 - ETA: 1:04 - loss: 0.2726 - acc: 0.885 - ETA: 1:03 - loss: 0.2726 - acc: 0.885 - ETA: 1:02 - loss: 0.2724 - acc: 0.885 - ETA: 1:01 - loss: 0.2717 - acc: 0.886 - ETA: 1:00 - loss: 0.2731 - acc: 0.885 - ETA: 59s - loss: 0.2729 - acc: 0.885 - ETA: 58s - loss: 0.2726 - acc: 0.88 - ETA: 57s - loss: 0.2722 - acc: 0.88 - ETA: 56s - loss: 0.2726 - acc: 0.88 - ETA: 55s - loss: 0.2723 - acc: 0.88 - ETA: 54s - loss: 0.2717 - acc: 0.88 - ETA: 53s - loss: 0.2718 - acc: 0.88 - ETA: 52s - loss: 0.2718 - acc: 0.88 - ETA: 51s - loss: 0.2715 - acc: 0.88 - ETA: 50s - loss: 0.2707 - acc: 0.88 - ETA: 49s - loss: 0.2719 - acc: 0.88 - ETA: 48s - loss: 0.2716 - acc: 0.88 - ETA: 46s - loss: 0.2710 - acc: 0.88 - ETA: 45s - loss: 0.2712 - acc: 0.88 - ETA: 44s - loss: 0.2719 - acc: 0.88 - ETA: 43s - loss: 0.2718 - acc: 0.88 - ETA: 42s - loss: 0.2712 - acc: 0.88 - ETA: 41s - loss: 0.2712 - acc: 0.88 - ETA: 40s - loss: 0.2710 - acc: 0.88 - ETA: 39s - loss: 0.2704 - acc: 0.88 - ETA: 38s - loss: 0.2707 - acc: 0.88 - ETA: 37s - loss: 0.2698 - acc: 0.88 - ETA: 36s - loss: 0.2696 - acc: 0.88 - ETA: 35s - loss: 0.2693 - acc: 0.88 - ETA: 34s - loss: 0.2688 - acc: 0.88 - ETA: 33s - loss: 0.2685 - acc: 0.88 - ETA: 32s - loss: 0.2691 - acc: 0.88 - ETA: 31s - loss: 0.2699 - acc: 0.88 - ETA: 30s - loss: 0.2696 - acc: 0.88 - ETA: 29s - loss: 0.2699 - acc: 0.88 - ETA: 28s - loss: 0.2700 - acc: 0.88 - ETA: 27s - loss: 0.2694 - acc: 0.88 - ETA: 26s - loss: 0.2694 - acc: 0.88 - ETA: 25s - loss: 0.2689 - acc: 0.88 - ETA: 24s - loss: 0.2687 - acc: 0.88 - ETA: 23s - loss: 0.2699 - acc: 0.88 - ETA: 22s - loss: 0.2706 - acc: 0.88 - ETA: 21s - loss: 0.2712 - acc: 0.88 - ETA: 20s - loss: 0.2711 - acc: 0.88 - ETA: 19s - loss: 0.2712 - acc: 0.88 - ETA: 18s - loss: 0.2713 - acc: 0.88 - ETA: 17s - loss: 0.2712 - acc: 0.88 - ETA: 16s - loss: 0.2714 - acc: 0.88 - ETA: 15s - loss: 0.2713 - acc: 0.88 - ETA: 14s - loss: 0.2714 - acc: 0.88 - ETA: 13s - loss: 0.2710 - acc: 0.88 - ETA: 12s - loss: 0.2703 - acc: 0.88 - ETA: 11s - loss: 0.2703 - acc: 0.88 - ETA: 10s - loss: 0.2702 - acc: 0.88 - ETA: 9s - loss: 0.2700 - acc: 0.8881 - ETA: 8s - loss: 0.2705 - acc: 0.887 - ETA: 7s - loss: 0.2702 - acc: 0.887 - ETA: 6s - loss: 0.2705 - acc: 0.887 - ETA: 5s - loss: 0.2708 - acc: 0.887 - ETA: 4s - loss: 0.2706 - acc: 0.887 - ETA: 3s - loss: 0.2702 - acc: 0.887 - ETA: 2s - loss: 0.2712 - acc: 0.887 - ETA: 1s - loss: 0.2707 - acc: 0.887 - 600s 117ms/step - loss: 0.2705 - acc: 0.8879 - val_loss: 0.2678 - val_acc: 0.8904\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3264/5120 [==================>...........] - ETA: 5:27 - loss: 0.1137 - acc: 1.000 - ETA: 5:25 - loss: 0.1398 - acc: 0.968 - ETA: 5:23 - loss: 0.1576 - acc: 0.958 - ETA: 5:23 - loss: 0.1541 - acc: 0.968 - ETA: 5:21 - loss: 0.1649 - acc: 0.962 - ETA: 5:21 - loss: 0.1746 - acc: 0.947 - ETA: 5:20 - loss: 0.2304 - acc: 0.928 - ETA: 5:19 - loss: 0.2194 - acc: 0.937 - ETA: 5:17 - loss: 0.2126 - acc: 0.937 - ETA: 5:16 - loss: 0.2343 - acc: 0.937 - ETA: 5:15 - loss: 0.2230 - acc: 0.937 - ETA: 5:14 - loss: 0.2526 - acc: 0.927 - ETA: 5:12 - loss: 0.2621 - acc: 0.923 - ETA: 5:11 - loss: 0.2483 - acc: 0.928 - ETA: 5:10 - loss: 0.2353 - acc: 0.933 - ETA: 5:09 - loss: 0.2234 - acc: 0.937 - ETA: 5:08 - loss: 0.2164 - acc: 0.941 - ETA: 5:07 - loss: 0.2282 - acc: 0.937 - ETA: 5:06 - loss: 0.2221 - acc: 0.940 - ETA: 5:05 - loss: 0.2149 - acc: 0.943 - ETA: 5:04 - loss: 0.2117 - acc: 0.943 - ETA: 5:03 - loss: 0.2037 - acc: 0.946 - ETA: 5:02 - loss: 0.2007 - acc: 0.942 - ETA: 5:01 - loss: 0.2029 - acc: 0.942 - ETA: 5:00 - loss: 0.1982 - acc: 0.945 - ETA: 4:59 - loss: 0.1937 - acc: 0.947 - ETA: 4:58 - loss: 0.1905 - acc: 0.946 - ETA: 4:57 - loss: 0.1959 - acc: 0.946 - ETA: 4:56 - loss: 0.1926 - acc: 0.946 - ETA: 4:55 - loss: 0.1961 - acc: 0.945 - ETA: 4:54 - loss: 0.1958 - acc: 0.945 - ETA: 4:53 - loss: 0.1944 - acc: 0.945 - ETA: 4:52 - loss: 0.1924 - acc: 0.945 - ETA: 4:51 - loss: 0.1893 - acc: 0.944 - ETA: 4:50 - loss: 0.1868 - acc: 0.944 - ETA: 4:49 - loss: 0.1850 - acc: 0.944 - ETA: 4:48 - loss: 0.1809 - acc: 0.945 - ETA: 4:47 - loss: 0.1801 - acc: 0.945 - ETA: 4:46 - loss: 0.1815 - acc: 0.945 - ETA: 4:45 - loss: 0.1816 - acc: 0.945 - ETA: 4:44 - loss: 0.1792 - acc: 0.945 - ETA: 4:43 - loss: 0.1780 - acc: 0.944 - ETA: 4:42 - loss: 0.1753 - acc: 0.946 - ETA: 4:41 - loss: 0.1721 - acc: 0.947 - ETA: 4:40 - loss: 0.1717 - acc: 0.945 - ETA: 4:39 - loss: 0.1714 - acc: 0.945 - ETA: 4:38 - loss: 0.1786 - acc: 0.941 - ETA: 4:37 - loss: 0.1809 - acc: 0.940 - ETA: 4:36 - loss: 0.1774 - acc: 0.941 - ETA: 4:35 - loss: 0.1794 - acc: 0.940 - ETA: 4:34 - loss: 0.1829 - acc: 0.938 - ETA: 4:33 - loss: 0.1802 - acc: 0.939 - ETA: 4:32 - loss: 0.1779 - acc: 0.941 - ETA: 4:31 - loss: 0.1767 - acc: 0.941 - ETA: 4:30 - loss: 0.1747 - acc: 0.942 - ETA: 4:29 - loss: 0.1729 - acc: 0.942 - ETA: 4:28 - loss: 0.1708 - acc: 0.943 - ETA: 4:27 - loss: 0.1684 - acc: 0.944 - ETA: 4:26 - loss: 0.1676 - acc: 0.944 - ETA: 4:25 - loss: 0.1704 - acc: 0.944 - ETA: 4:24 - loss: 0.1681 - acc: 0.945 - ETA: 4:23 - loss: 0.1679 - acc: 0.945 - ETA: 4:22 - loss: 0.1662 - acc: 0.946 - ETA: 4:21 - loss: 0.1647 - acc: 0.947 - ETA: 4:20 - loss: 0.1640 - acc: 0.947 - ETA: 4:19 - loss: 0.1630 - acc: 0.947 - ETA: 4:18 - loss: 0.1638 - acc: 0.945 - ETA: 4:17 - loss: 0.1619 - acc: 0.946 - ETA: 4:16 - loss: 0.1624 - acc: 0.946 - ETA: 4:15 - loss: 0.1610 - acc: 0.947 - ETA: 4:14 - loss: 0.1638 - acc: 0.946 - ETA: 4:13 - loss: 0.1617 - acc: 0.947 - ETA: 4:12 - loss: 0.1633 - acc: 0.946 - ETA: 4:11 - loss: 0.1624 - acc: 0.945 - ETA: 4:10 - loss: 0.1625 - acc: 0.945 - ETA: 4:09 - loss: 0.1635 - acc: 0.944 - ETA: 4:08 - loss: 0.1626 - acc: 0.945 - ETA: 4:06 - loss: 0.1611 - acc: 0.946 - ETA: 4:05 - loss: 0.1620 - acc: 0.946 - ETA: 4:04 - loss: 0.1616 - acc: 0.946 - ETA: 4:03 - loss: 0.1597 - acc: 0.946 - ETA: 4:02 - loss: 0.1604 - acc: 0.946 - ETA: 4:01 - loss: 0.1602 - acc: 0.946 - ETA: 4:00 - loss: 0.1587 - acc: 0.947 - ETA: 3:59 - loss: 0.1581 - acc: 0.947 - ETA: 3:58 - loss: 0.1599 - acc: 0.946 - ETA: 3:57 - loss: 0.1584 - acc: 0.947 - ETA: 3:56 - loss: 0.1579 - acc: 0.947 - ETA: 3:55 - loss: 0.1564 - acc: 0.948 - ETA: 3:54 - loss: 0.1560 - acc: 0.947 - ETA: 3:53 - loss: 0.1554 - acc: 0.947 - ETA: 3:52 - loss: 0.1541 - acc: 0.948 - ETA: 3:51 - loss: 0.1528 - acc: 0.948 - ETA: 3:50 - loss: 0.1517 - acc: 0.949 - ETA: 3:49 - loss: 0.1516 - acc: 0.949 - ETA: 3:48 - loss: 0.1501 - acc: 0.949 - ETA: 3:47 - loss: 0.1498 - acc: 0.949 - ETA: 3:46 - loss: 0.1484 - acc: 0.950 - ETA: 3:45 - loss: 0.1484 - acc: 0.950 - ETA: 3:44 - loss: 0.1472 - acc: 0.950 - ETA: 3:43 - loss: 0.1494 - acc: 0.949 - ETA: 3:42 - loss: 0.1500 - acc: 0.949 - ETA: 3:41 - loss: 0.1493 - acc: 0.949 - ETA: 3:40 - loss: 0.1479 - acc: 0.949 - ETA: 3:39 - loss: 0.1492 - acc: 0.949 - ETA: 3:38 - loss: 0.1492 - acc: 0.949 - ETA: 3:37 - loss: 0.1492 - acc: 0.949 - ETA: 3:36 - loss: 0.1508 - acc: 0.949 - ETA: 3:35 - loss: 0.1498 - acc: 0.949 - ETA: 3:34 - loss: 0.1532 - acc: 0.947 - ETA: 3:33 - loss: 0.1548 - acc: 0.947 - ETA: 3:32 - loss: 0.1546 - acc: 0.947 - ETA: 3:31 - loss: 0.1539 - acc: 0.948 - ETA: 3:30 - loss: 0.1535 - acc: 0.947 - ETA: 3:29 - loss: 0.1584 - acc: 0.946 - ETA: 3:28 - loss: 0.1595 - acc: 0.945 - ETA: 3:27 - loss: 0.1597 - acc: 0.945 - ETA: 3:26 - loss: 0.1592 - acc: 0.945 - ETA: 3:25 - loss: 0.1592 - acc: 0.945 - ETA: 3:24 - loss: 0.1613 - acc: 0.944 - ETA: 3:23 - loss: 0.1612 - acc: 0.944 - ETA: 3:22 - loss: 0.1609 - acc: 0.944 - ETA: 3:21 - loss: 0.1617 - acc: 0.944 - ETA: 3:20 - loss: 0.1618 - acc: 0.944 - ETA: 3:19 - loss: 0.1623 - acc: 0.944 - ETA: 3:17 - loss: 0.1618 - acc: 0.944 - ETA: 3:16 - loss: 0.1627 - acc: 0.943 - ETA: 3:15 - loss: 0.1643 - acc: 0.942 - ETA: 3:14 - loss: 0.1656 - acc: 0.942 - ETA: 3:13 - loss: 0.1666 - acc: 0.941 - ETA: 3:12 - loss: 0.1658 - acc: 0.942 - ETA: 3:11 - loss: 0.1698 - acc: 0.941 - ETA: 3:10 - loss: 0.1692 - acc: 0.941 - ETA: 3:09 - loss: 0.1686 - acc: 0.942 - ETA: 3:08 - loss: 0.1686 - acc: 0.941 - ETA: 3:07 - loss: 0.1677 - acc: 0.942 - ETA: 3:06 - loss: 0.1672 - acc: 0.942 - ETA: 3:05 - loss: 0.1667 - acc: 0.942 - ETA: 3:04 - loss: 0.1658 - acc: 0.943 - ETA: 3:03 - loss: 0.1662 - acc: 0.942 - ETA: 3:02 - loss: 0.1680 - acc: 0.942 - ETA: 3:01 - loss: 0.1672 - acc: 0.942 - ETA: 3:00 - loss: 0.1685 - acc: 0.942 - ETA: 2:59 - loss: 0.1677 - acc: 0.943 - ETA: 2:58 - loss: 0.1670 - acc: 0.943 - ETA: 2:57 - loss: 0.1661 - acc: 0.943 - ETA: 2:56 - loss: 0.1658 - acc: 0.943 - ETA: 2:55 - loss: 0.1648 - acc: 0.943 - ETA: 2:54 - loss: 0.1649 - acc: 0.943 - ETA: 2:53 - loss: 0.1657 - acc: 0.943 - ETA: 2:52 - loss: 0.1662 - acc: 0.942 - ETA: 2:51 - loss: 0.1652 - acc: 0.943 - ETA: 2:50 - loss: 0.1679 - acc: 0.942 - ETA: 2:49 - loss: 0.1675 - acc: 0.942 - ETA: 2:48 - loss: 0.1697 - acc: 0.941 - ETA: 2:47 - loss: 0.1705 - acc: 0.941 - ETA: 2:46 - loss: 0.1699 - acc: 0.941 - ETA: 2:45 - loss: 0.1708 - acc: 0.941 - ETA: 2:44 - loss: 0.1699 - acc: 0.941 - ETA: 2:43 - loss: 0.1704 - acc: 0.941 - ETA: 2:42 - loss: 0.1705 - acc: 0.941 - ETA: 2:41 - loss: 0.1712 - acc: 0.940 - ETA: 2:40 - loss: 0.1727 - acc: 0.939 - ETA: 2:39 - loss: 0.1744 - acc: 0.939 - ETA: 2:38 - loss: 0.1748 - acc: 0.939 - ETA: 2:37 - loss: 0.1740 - acc: 0.939 - ETA: 2:36 - loss: 0.1735 - acc: 0.939 - ETA: 2:35 - loss: 0.1735 - acc: 0.939 - ETA: 2:34 - loss: 0.1734 - acc: 0.939 - ETA: 2:33 - loss: 0.1740 - acc: 0.939 - ETA: 2:32 - loss: 0.1748 - acc: 0.938 - ETA: 2:31 - loss: 0.1756 - acc: 0.937 - ETA: 2:30 - loss: 0.1768 - acc: 0.937 - ETA: 2:29 - loss: 0.1761 - acc: 0.937 - ETA: 2:28 - loss: 0.1777 - acc: 0.936 - ETA: 2:26 - loss: 0.1781 - acc: 0.936 - ETA: 2:25 - loss: 0.1797 - acc: 0.935 - ETA: 2:24 - loss: 0.1804 - acc: 0.935 - ETA: 2:23 - loss: 0.1804 - acc: 0.935 - ETA: 2:22 - loss: 0.1815 - acc: 0.934 - ETA: 2:21 - loss: 0.1813 - acc: 0.934 - ETA: 2:20 - loss: 0.1813 - acc: 0.934 - ETA: 2:19 - loss: 0.1811 - acc: 0.934 - ETA: 2:18 - loss: 0.1821 - acc: 0.934 - ETA: 2:17 - loss: 0.1814 - acc: 0.934 - ETA: 2:16 - loss: 0.1818 - acc: 0.933 - ETA: 2:15 - loss: 0.1824 - acc: 0.933 - ETA: 2:14 - loss: 0.1821 - acc: 0.933 - ETA: 2:13 - loss: 0.1816 - acc: 0.934 - ETA: 2:12 - loss: 0.1815 - acc: 0.934 - ETA: 2:11 - loss: 0.1821 - acc: 0.933 - ETA: 2:10 - loss: 0.1827 - acc: 0.933 - ETA: 2:09 - loss: 0.1841 - acc: 0.932 - ETA: 2:08 - loss: 0.1837 - acc: 0.933 - ETA: 2:07 - loss: 0.1831 - acc: 0.933 - ETA: 2:06 - loss: 0.1827 - acc: 0.933 - ETA: 2:05 - loss: 0.1837 - acc: 0.932 - ETA: 2:04 - loss: 0.1832 - acc: 0.932 - ETA: 2:03 - loss: 0.1834 - acc: 0.932 - ETA: 2:02 - loss: 0.1833 - acc: 0.932 - ETA: 2:01 - loss: 0.1829 - acc: 0.932 - ETA: 2:00 - loss: 0.1830 - acc: 0.932 - ETA: 1:59 - loss: 0.1835 - acc: 0.932 - ETA: 1:58 - loss: 0.1834 - acc: 0.9326"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5120/5120 [==============================] - ETA: 1:57 - loss: 0.1831 - acc: 0.932 - ETA: 1:56 - loss: 0.1826 - acc: 0.933 - ETA: 1:55 - loss: 0.1820 - acc: 0.933 - ETA: 1:54 - loss: 0.1820 - acc: 0.933 - ETA: 1:53 - loss: 0.1815 - acc: 0.933 - ETA: 1:52 - loss: 0.1811 - acc: 0.933 - ETA: 1:51 - loss: 0.1815 - acc: 0.933 - ETA: 1:50 - loss: 0.1815 - acc: 0.933 - ETA: 1:49 - loss: 0.1823 - acc: 0.932 - ETA: 1:48 - loss: 0.1822 - acc: 0.932 - ETA: 1:47 - loss: 0.1816 - acc: 0.932 - ETA: 1:46 - loss: 0.1828 - acc: 0.932 - ETA: 1:45 - loss: 0.1821 - acc: 0.932 - ETA: 1:44 - loss: 0.1821 - acc: 0.932 - ETA: 1:43 - loss: 0.1819 - acc: 0.932 - ETA: 1:42 - loss: 0.1818 - acc: 0.932 - ETA: 1:41 - loss: 0.1821 - acc: 0.932 - ETA: 1:40 - loss: 0.1817 - acc: 0.932 - ETA: 1:38 - loss: 0.1826 - acc: 0.932 - ETA: 1:37 - loss: 0.1834 - acc: 0.932 - ETA: 1:36 - loss: 0.1831 - acc: 0.932 - ETA: 1:35 - loss: 0.1828 - acc: 0.932 - ETA: 1:34 - loss: 0.1842 - acc: 0.932 - ETA: 1:33 - loss: 0.1843 - acc: 0.932 - ETA: 1:32 - loss: 0.1842 - acc: 0.932 - ETA: 1:31 - loss: 0.1844 - acc: 0.932 - ETA: 1:30 - loss: 0.1839 - acc: 0.932 - ETA: 1:29 - loss: 0.1836 - acc: 0.932 - ETA: 1:28 - loss: 0.1837 - acc: 0.932 - ETA: 1:27 - loss: 0.1837 - acc: 0.932 - ETA: 1:26 - loss: 0.1832 - acc: 0.932 - ETA: 1:25 - loss: 0.1826 - acc: 0.932 - ETA: 1:24 - loss: 0.1823 - acc: 0.932 - ETA: 1:23 - loss: 0.1819 - acc: 0.933 - ETA: 1:22 - loss: 0.1831 - acc: 0.932 - ETA: 1:21 - loss: 0.1827 - acc: 0.932 - ETA: 1:20 - loss: 0.1821 - acc: 0.932 - ETA: 1:19 - loss: 0.1818 - acc: 0.932 - ETA: 1:18 - loss: 0.1823 - acc: 0.932 - ETA: 1:17 - loss: 0.1818 - acc: 0.932 - ETA: 1:16 - loss: 0.1829 - acc: 0.932 - ETA: 1:15 - loss: 0.1825 - acc: 0.932 - ETA: 1:14 - loss: 0.1822 - acc: 0.932 - ETA: 1:13 - loss: 0.1823 - acc: 0.932 - ETA: 1:12 - loss: 0.1818 - acc: 0.932 - ETA: 1:11 - loss: 0.1815 - acc: 0.932 - ETA: 1:10 - loss: 0.1816 - acc: 0.932 - ETA: 1:09 - loss: 0.1815 - acc: 0.932 - ETA: 1:08 - loss: 0.1820 - acc: 0.932 - ETA: 1:07 - loss: 0.1817 - acc: 0.932 - ETA: 1:06 - loss: 0.1811 - acc: 0.932 - ETA: 1:05 - loss: 0.1804 - acc: 0.933 - ETA: 1:04 - loss: 0.1799 - acc: 0.933 - ETA: 1:03 - loss: 0.1798 - acc: 0.932 - ETA: 1:02 - loss: 0.1798 - acc: 0.932 - ETA: 1:01 - loss: 0.1804 - acc: 0.932 - ETA: 1:00 - loss: 0.1807 - acc: 0.932 - ETA: 59s - loss: 0.1812 - acc: 0.932 - ETA: 58s - loss: 0.1823 - acc: 0.93 - ETA: 57s - loss: 0.1818 - acc: 0.93 - ETA: 56s - loss: 0.1818 - acc: 0.93 - ETA: 55s - loss: 0.1835 - acc: 0.93 - ETA: 54s - loss: 0.1839 - acc: 0.93 - ETA: 53s - loss: 0.1837 - acc: 0.93 - ETA: 52s - loss: 0.1836 - acc: 0.93 - ETA: 51s - loss: 0.1842 - acc: 0.93 - ETA: 50s - loss: 0.1840 - acc: 0.93 - ETA: 49s - loss: 0.1836 - acc: 0.93 - ETA: 47s - loss: 0.1838 - acc: 0.93 - ETA: 46s - loss: 0.1836 - acc: 0.93 - ETA: 45s - loss: 0.1834 - acc: 0.93 - ETA: 44s - loss: 0.1834 - acc: 0.93 - ETA: 43s - loss: 0.1833 - acc: 0.93 - ETA: 42s - loss: 0.1839 - acc: 0.93 - ETA: 41s - loss: 0.1839 - acc: 0.93 - ETA: 40s - loss: 0.1854 - acc: 0.93 - ETA: 39s - loss: 0.1852 - acc: 0.93 - ETA: 38s - loss: 0.1854 - acc: 0.93 - ETA: 37s - loss: 0.1856 - acc: 0.93 - ETA: 36s - loss: 0.1859 - acc: 0.92 - ETA: 35s - loss: 0.1863 - acc: 0.92 - ETA: 34s - loss: 0.1866 - acc: 0.92 - ETA: 33s - loss: 0.1862 - acc: 0.92 - ETA: 32s - loss: 0.1869 - acc: 0.92 - ETA: 31s - loss: 0.1868 - acc: 0.92 - ETA: 30s - loss: 0.1864 - acc: 0.92 - ETA: 29s - loss: 0.1873 - acc: 0.92 - ETA: 28s - loss: 0.1871 - acc: 0.92 - ETA: 27s - loss: 0.1867 - acc: 0.92 - ETA: 26s - loss: 0.1866 - acc: 0.92 - ETA: 25s - loss: 0.1864 - acc: 0.92 - ETA: 24s - loss: 0.1862 - acc: 0.92 - ETA: 23s - loss: 0.1862 - acc: 0.92 - ETA: 22s - loss: 0.1859 - acc: 0.92 - ETA: 21s - loss: 0.1856 - acc: 0.92 - ETA: 20s - loss: 0.1853 - acc: 0.92 - ETA: 19s - loss: 0.1849 - acc: 0.93 - ETA: 18s - loss: 0.1845 - acc: 0.93 - ETA: 17s - loss: 0.1851 - acc: 0.92 - ETA: 16s - loss: 0.1847 - acc: 0.93 - ETA: 15s - loss: 0.1862 - acc: 0.92 - ETA: 14s - loss: 0.1858 - acc: 0.92 - ETA: 13s - loss: 0.1860 - acc: 0.92 - ETA: 12s - loss: 0.1855 - acc: 0.93 - ETA: 11s - loss: 0.1863 - acc: 0.93 - ETA: 10s - loss: 0.1860 - acc: 0.93 - ETA: 9s - loss: 0.1861 - acc: 0.9301 - ETA: 8s - loss: 0.1856 - acc: 0.930 - ETA: 7s - loss: 0.1866 - acc: 0.929 - ETA: 6s - loss: 0.1865 - acc: 0.929 - ETA: 5s - loss: 0.1862 - acc: 0.929 - ETA: 4s - loss: 0.1861 - acc: 0.929 - ETA: 3s - loss: 0.1860 - acc: 0.929 - ETA: 2s - loss: 0.1861 - acc: 0.929 - ETA: 1s - loss: 0.1862 - acc: 0.929 - 600s 117ms/step - loss: 0.1859 - acc: 0.9297 - val_loss: 0.3181 - val_acc: 0.8787\n"
     ]
    }
   ],
   "source": [
    "model = build_model(max_seq_length)\n",
    "\n",
    "initialize_vars(sess)\n",
    "\n",
    "model.fit(\n",
    "    [train_input_ids, train_input_masks, train_segment_ids], \n",
    "    train_labels,\n",
    "    validation_data=([test_input_ids, test_input_masks, test_segment_ids], test_labels),\n",
    "    epochs=3,\n",
    "    # GPU ResourceExhaustedError 뜨면 batch size 줄일 것\n",
    "    batch_size=16)\n",
    "\n",
    "model.save('BertModel_3epoch.h5')\n",
    "\n",
    "pre_save_preds = model.predict([test_input_ids[0:100], \n",
    "                            test_input_masks[0:100], \n",
    "                            test_segment_ids[0:100]]\n",
    "                          ) # predictions before we clear and reload model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qMWiDtpyQSoU"
   },
   "source": [
    "To start, we'll need to load a vocabulary file and lowercasing information directly from the BERT tf hub module:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MXkRiEBUqN3n"
   },
   "source": [
    "Voila! We have a sentiment classifier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "ERkTE8-7oQLZ",
    "outputId": "26c33224-dc2c-4b3d-f7b4-ac3ef0a58b27",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0505 22:33:07.000718 10548 tf_logging.py:115] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0505 22:33:08.590467 10548 tf_logging.py:115] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert_layer_3 (BertLayer)        (None, 768)          110104890   input_ids[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 256)          196864      bert_layer_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1)            257         dense_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 110,302,011\n",
      "Trainable params: 3,147,009\n",
      "Non-trainable params: 107,155,002\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Clear and load model\n",
    "model = None\n",
    "model = build_model(max_seq_length)\n",
    "initialize_vars(sess)\n",
    "model.load_weights('BertModel_3epoch.h5')\n",
    "\n",
    "post_save_preds = model.predict([test_input_ids[0:100], \n",
    "                                test_input_masks[0:100], \n",
    "                                test_segment_ids[0:100]]\n",
    "                              ) # predictions after we clear and reload model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.4457862e-03],\n",
       "       [9.6813858e-01],\n",
       "       [9.8280299e-01],\n",
       "       [7.5745098e-02],\n",
       "       [4.6677220e-01],\n",
       "       [9.9224585e-01],\n",
       "       [9.5921111e-01],\n",
       "       [6.4939745e-03],\n",
       "       [7.4180928e-03],\n",
       "       [9.4558853e-01],\n",
       "       [9.9092382e-01],\n",
       "       [9.7183630e-02],\n",
       "       [5.3208077e-01],\n",
       "       [9.9250728e-01],\n",
       "       [9.5702046e-01],\n",
       "       [1.6069709e-03],\n",
       "       [9.7688019e-01],\n",
       "       [1.0215268e-02],\n",
       "       [8.1035399e-01],\n",
       "       [8.0544382e-01],\n",
       "       [7.7812260e-01],\n",
       "       [9.7519207e-01],\n",
       "       [1.3380524e-02],\n",
       "       [4.8946780e-03],\n",
       "       [9.9204761e-01],\n",
       "       [3.5089269e-01],\n",
       "       [4.5742360e-03],\n",
       "       [3.5204613e-03],\n",
       "       [8.0536211e-01],\n",
       "       [5.2231675e-01],\n",
       "       [7.5015584e-03],\n",
       "       [8.9300768e-03],\n",
       "       [9.4740337e-01],\n",
       "       [4.4505000e-03],\n",
       "       [9.5870096e-04],\n",
       "       [1.9042206e-01],\n",
       "       [9.3479329e-01],\n",
       "       [9.5588309e-01],\n",
       "       [8.3491793e-03],\n",
       "       [9.9396425e-01],\n",
       "       [2.7839243e-01],\n",
       "       [9.5595932e-01],\n",
       "       [9.7242492e-01],\n",
       "       [2.3731275e-03],\n",
       "       [9.9668002e-01],\n",
       "       [9.9696547e-01],\n",
       "       [1.7551187e-02],\n",
       "       [7.1222112e-03],\n",
       "       [9.8643339e-01],\n",
       "       [4.5570320e-01],\n",
       "       [9.8429340e-01],\n",
       "       [7.1740843e-04],\n",
       "       [6.4406878e-01],\n",
       "       [9.3308371e-01],\n",
       "       [7.1738206e-02],\n",
       "       [9.9809116e-01],\n",
       "       [6.3018203e-03],\n",
       "       [9.9551207e-01],\n",
       "       [9.6047133e-01],\n",
       "       [4.1404709e-02],\n",
       "       [9.7723264e-01],\n",
       "       [3.8718856e-03],\n",
       "       [6.3402042e-02],\n",
       "       [9.8427421e-01],\n",
       "       [9.9374419e-01],\n",
       "       [8.9010143e-01],\n",
       "       [3.1634983e-01],\n",
       "       [9.8358005e-01],\n",
       "       [9.6205312e-01],\n",
       "       [8.4219879e-04],\n",
       "       [6.2511700e-01],\n",
       "       [9.9482429e-01],\n",
       "       [9.8609585e-01],\n",
       "       [9.8655486e-01],\n",
       "       [9.9525154e-01],\n",
       "       [9.9088556e-01],\n",
       "       [1.3786232e-01],\n",
       "       [9.8866487e-01],\n",
       "       [9.6446103e-01],\n",
       "       [9.8617089e-01],\n",
       "       [9.9687791e-01],\n",
       "       [2.6886554e-03],\n",
       "       [7.6914823e-01],\n",
       "       [9.8603708e-01],\n",
       "       [8.5479409e-01],\n",
       "       [9.9766064e-01],\n",
       "       [9.0550888e-01],\n",
       "       [8.2798637e-03],\n",
       "       [2.0671630e-02],\n",
       "       [3.4941286e-03],\n",
       "       [4.3384402e-04],\n",
       "       [9.6153682e-01],\n",
       "       [9.7054714e-01],\n",
       "       [9.8887151e-01],\n",
       "       [2.7327826e-02],\n",
       "       [9.8865509e-01],\n",
       "       [1.3986130e-03],\n",
       "       [6.5888554e-01],\n",
       "       [9.8745173e-01],\n",
       "       [9.7206998e-01]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_save_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Predicting Movie Reviews with BERT on TF Hub.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
